"""Report generation for Kakao analysis results"""

import pandas as pd
from typing import Dict, List, Any, Optional
from pathlib import Path
from datetime import datetime
import json
import logging

from .utils import format_duration


class ReportGenerator:
    """Generate analysis reports in markdown format"""
    
    def __init__(self, config, logger: logging.Logger = None):
        self.config = config
        self.logger = logger or logging.getLogger(__name__)
    
    def generate_analysis_report(self, analysis_results: Dict[str, Any], 
                               output_path: Path, viz_files: Dict[str, str] = None) -> str:
        """Generate comprehensive analysis report"""
        
        self.logger.info("Generating analysis report...")
        
        report_sections = []
        
        # Header
        report_sections.append(self._generate_header(analysis_results))
        
        # Executive Summary
        report_sections.append(self._generate_executive_summary(analysis_results))
        
        # Data Overview
        report_sections.append(self._generate_data_overview(analysis_results))
        
        # Basic Statistics
        report_sections.append(self._generate_basic_stats_section(analysis_results))
        
        # Conversation Dynamics
        report_sections.append(self._generate_conversation_dynamics(analysis_results))
        
        # Topic Analysis
        report_sections.append(self._generate_topic_analysis(analysis_results))
        
        # Fun Metrics
        report_sections.append(self._generate_fun_metrics_section(analysis_results))
        
        # Mention Analysis (KakaoTalk feature)
        if 'mention_analysis' in analysis_results:
            report_sections.append(self._generate_mention_analysis_section(analysis_results))
            
        # Context Flow Analysis  
        if 'mention_analysis' in analysis_results and 'context_analysis' in analysis_results['mention_analysis']:
            report_sections.append(self._generate_context_flow_section(analysis_results))
            
        # Topic Sentiment Analysis
        if 'topic_sentiment' in analysis_results:
            report_sections.append(self._generate_topic_sentiment_section(analysis_results))
        
        # Advanced AI Analysis (SPLADE, Ollama)
        if analysis_results.get('sparse_index_info') or analysis_results.get('advanced_topics'):
            report_sections.append(self._generate_advanced_ai_section(analysis_results))
        
        # Visualizations
        if viz_files:
            report_sections.append(self._generate_visualizations_section(viz_files))
        
        # Technical Details
        report_sections.append(self._generate_technical_details(analysis_results))
        
        # Insights and Recommendations
        report_sections.append(self._generate_insights_section(analysis_results))
        
        # Footer
        report_sections.append(self._generate_footer())
        
        # Combine all sections
        full_report = '\n\n'.join(report_sections)
        
        # Save report
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(full_report)
        
        self.logger.info(f"Analysis report saved to {output_path}")
        return str(output_path)
    
    def _generate_header(self, analysis_results: Dict[str, Any]) -> str:
        """Generate report header"""
        
        return f"""# ì¹´ì¹´ì˜¤í†¡ ëŒ€í™” ë¶„ì„ ë¦¬í¬íŠ¸
        
**ìƒì„± ì¼ì‹œ**: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M')}  
**ë¶„ì„ ë„êµ¬**: Kakao Analyzer v{analysis_results.get('version', '1.0.0')}  
**ë¶„ì„ ëŒ€ìƒ**: {analysis_results.get('data_info', {}).get('source_file', 'Unknown')}

---"""
    
    def _generate_executive_summary(self, analysis_results: Dict[str, Any]) -> str:
        """Generate executive summary"""
        
        basic_stats = analysis_results.get('basic_stats', {})
        summary_stats = basic_stats.get('summary', {})
        
        if not summary_stats or 'error' in summary_stats:
            return "## ğŸ“Š ìš”ì•½\n\n*ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.*"
        
        total_messages = summary_stats.get('total_messages', 0)
        unique_users = summary_stats.get('unique_users', 0)
        date_range = summary_stats.get('date_range', {})
        activity_summary = summary_stats.get('activity_summary', {})
        
        insights = self._extract_key_insights(analysis_results)
        
        return f"""## ğŸ“Š ìš”ì•½

### í•µì‹¬ ì§€í‘œ
- **ì´ ë©”ì‹œì§€ ìˆ˜**: {total_messages:,}ê°œ
- **ì°¸ì—¬ì ìˆ˜**: {unique_users}ëª…
- **ë¶„ì„ ê¸°ê°„**: {date_range.get('total_days', 0)}ì¼ ({date_range.get('start', '')[:10]} ~ {date_range.get('end', '')[:10]})
- **ì¼í‰ê·  ë©”ì‹œì§€**: {activity_summary.get('messages_per_day', 0):.1f}ê°œ

### ì£¼ìš” ì¸ì‚¬ì´íŠ¸
{chr(10).join([f"- {insight}" for insight in insights[:5]])}"""
    
    def _generate_data_overview(self, analysis_results: Dict[str, Any]) -> str:
        """Generate data overview section"""
        
        data_info = analysis_results.get('data_info', {})
        validation = analysis_results.get('data_validation', {})
        
        overview_text = f"""## ğŸ“‹ ë°ì´í„° ê°œìš”

### ë°ì´í„° í’ˆì§ˆ
- **ìƒíƒœ**: {'âœ… ì–‘í˜¸' if validation.get('is_valid', False) else 'âš ï¸ ë¬¸ì œ ìˆìŒ'}
- **ê²½ê³ ì‚¬í•­**: {len(validation.get('warnings', []))}ê°œ
- **ì˜¤ë¥˜ì‚¬í•­**: {len(validation.get('errors', []))}ê°œ"""
        
        if validation.get('warnings'):
            overview_text += f"\n\n**ê²½ê³ ì‚¬í•­:**\n" + '\n'.join([f"- {warning}" for warning in validation['warnings'][:3]])
        
        if validation.get('errors'):
            overview_text += f"\n\n**ì˜¤ë¥˜ì‚¬í•­:**\n" + '\n'.join([f"- {error}" for error in validation['errors'][:3]])
        
        return overview_text
    
    def _generate_basic_stats_section(self, analysis_results: Dict[str, Any]) -> str:
        """Generate basic statistics section"""
        
        basic_stats = analysis_results.get('basic_stats', {})
        per_user = basic_stats.get('per_user', [])
        temporal = basic_stats.get('temporal', {})
        
        section = "## ğŸ“ˆ ê¸°ë³¸ í†µê³„"
        
        # User statistics
        if per_user:
            section += "\n\n### ì‚¬ìš©ìë³„ í™œë™"
            section += "\n\n| ì‚¬ìš©ì | ë©”ì‹œì§€ ìˆ˜ | ì°¸ì—¬ìœ¨ | ì¼í‰ê·  | ë‹¨ì–´ìˆ˜ |"
            section += "\n|--------|-----------|--------|--------|--------|"
            
            for user_stat in per_user[:10]:  # Top 10 users
                section += f"\n| {user_stat['user']} | {user_stat['total_messages']:,} | {user_stat['participation_ratio']:.1f}% | {user_stat['messages_per_day']:.1f} | {user_stat['total_words']:,} |"
        
        # Temporal patterns
        if temporal:
            peak_activity = temporal.get('peak_activity', {})
            section += f"\n\n### ì‹œê°„ì  íŒ¨í„´"
            section += f"\n- **ìµœê³  í™œë™ ì‹œê°„**: {peak_activity.get('peak_hour', 'N/A')}"
            section += f"\n- **ìµœê³  í™œë™ ìš”ì¼**: {peak_activity.get('peak_day', 'N/A')}"
            
            time_periods = temporal.get('time_period_distribution', {})
            if time_periods:
                section += f"\n- **ì•„ì¹¨ (06-11ì‹œ)**: {time_periods.get('morning_6_11', {}).get('percent', 0):.1f}%"
                section += f"\n- **ì˜¤í›„ (12-17ì‹œ)**: {time_periods.get('afternoon_12_17', {}).get('percent', 0):.1f}%"
                section += f"\n- **ì €ë… (18-23ì‹œ)**: {time_periods.get('evening_18_23', {}).get('percent', 0):.1f}%"
                section += f"\n- **ìƒˆë²½ (00-05ì‹œ)**: {time_periods.get('night_0_5', {}).get('percent', 0):.1f}%"
        
        # Message characteristics
        summary_stats = basic_stats.get('summary', {})
        message_chars = summary_stats.get('message_characteristics', {})
        if message_chars:
            section += f"\n\n### ë©”ì‹œì§€ íŠ¹ì„±"
            section += f"\n- **í‰ê·  ë©”ì‹œì§€ ê¸¸ì´**: {message_chars.get('avg_length', 0):.1f}ì"
            section += f"\n- **í‰ê·  ë‹¨ì–´ ìˆ˜ (ì›ë³¸)**: {message_chars.get('avg_words', 0):.1f}ê°œ"
            if 'avg_filtered_words' in message_chars:
                section += f"\n- **í‰ê·  ë‹¨ì–´ ìˆ˜ (í•„í„°ë§)**: {message_chars.get('avg_filtered_words', 0):.1f}ê°œ"
            section += f"\n- **ì¤‘ê°„ê°’ ë©”ì‹œì§€ ê¸¸ì´**: {message_chars.get('median_length', 0)}ì"
            section += f"\n- **ì¤‘ê°„ê°’ ë‹¨ì–´ ìˆ˜ (ì›ë³¸)**: {message_chars.get('median_words', 0)}ê°œ"
            if 'median_filtered_words' in message_chars:
                section += f"\n- **ì¤‘ê°„ê°’ ë‹¨ì–´ ìˆ˜ (í•„í„°ë§)**: {message_chars.get('median_filtered_words', 0)}ê°œ"
            section += f"\n- **ìµœëŒ€ ë©”ì‹œì§€ ê¸¸ì´**: {message_chars.get('max_length', 0)}ì"
            section += f"\n- **ì´ ë‹¨ì–´ ìˆ˜**: {message_chars.get('total_words', 0):,}ê°œ"
            if 'total_filtered_words' in message_chars:
                section += f"\n- **ì´ í•„í„°ë§ëœ ë‹¨ì–´ ìˆ˜**: {message_chars.get('total_filtered_words', 0):,}ê°œ"
        
        return section
    
    def _generate_conversation_dynamics(self, analysis_results: Dict[str, Any]) -> str:
        """Generate conversation dynamics section"""
        
        turn_analysis = analysis_results.get('turn_analysis', {})
        
        if not turn_analysis:
            return "## ğŸ’¬ ëŒ€í™” ì—­í•™\n\n*ëŒ€í™” ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.*"
        
        section = "## ğŸ’¬ ëŒ€í™” ì—­í•™"
        
        # Turn statistics
        section += f"\n\n### ëŒ€í™” í„´ ë¶„ì„"
        # Calculate statistics from turn list
        if isinstance(turn_analysis, list):
            turns = turn_analysis
            section += f"\n- **ì´ ëŒ€í™” í„´**: {len(turns)}ê°œ"
            
            if turns:
                # Calculate duration statistics
                durations = [turn.get('duration_minutes', 0) for turn in turns]
                avg_duration = sum(durations) / len(durations) if durations else 0
                max_duration = max(durations) if durations else 0
                
                section += f"\n- **í‰ê·  ëŒ€í™” ê¸¸ì´**: {avg_duration:.1f}ë¶„"
                section += f"\n- **ìµœì¥ ëŒ€í™” ê¸¸ì´**: {max_duration:.1f}ë¶„"
                
                # Calculate message statistics
                message_counts = [turn.get('message_count', 0) for turn in turns]
                avg_messages = sum(message_counts) / len(message_counts) if message_counts else 0
                section += f"\n- **í„´ë‹¹ í‰ê·  ë©”ì‹œì§€**: {avg_messages:.1f}ê°œ"
                
                # Find conversation initiators (first message in each turn)
                initiators = {}
                for turn in turns:
                    messages = turn.get('messages', [])
                    if messages:
                        first_user = messages[0].get('user', 'Unknown')
                        initiators[first_user] = initiators.get(first_user, 0) + 1
            else:
                initiators = {}
        else:
            # Fallback for dict structure
            section += f"\n- **ì´ ëŒ€í™” í„´**: {turn_analysis.get('total_turns', 0)}ê°œ"
            duration_stats = turn_analysis.get('duration_stats', {})
            if duration_stats:
                section += f"\n- **í‰ê·  ëŒ€í™” ê¸¸ì´**: {duration_stats.get('avg_duration_minutes', 0):.1f}ë¶„"
                section += f"\n- **ìµœì¥ ëŒ€í™” ê¸¸ì´**: {duration_stats.get('max_duration_minutes', 0):.1f}ë¶„"
            
            message_stats = turn_analysis.get('message_stats', {})
            if message_stats:
                section += f"\n- **í„´ë‹¹ í‰ê·  ë©”ì‹œì§€**: {message_stats.get('avg_messages_per_turn', 0):.1f}ê°œ"
            
            initiators = turn_analysis.get('conversation_initiators', {})
        if initiators:
            section += f"\n\n### ëŒ€í™” ì‹œì‘ íŒ¨í„´"
            # Sort initiators by count in descending order
            sorted_initiators = sorted(initiators.items(), key=lambda x: x[1], reverse=True)
            for user, count in sorted_initiators[:3]:
                section += f"\n- **{user}**: {count}íšŒ ì‹œì‘"
        
        return section
    
    def _generate_topic_analysis(self, analysis_results: Dict[str, Any]) -> str:
        """Generate topic analysis section"""
        
        topic_segments = analysis_results.get('topic_segments', [])
        topic_shifts = analysis_results.get('topic_analysis', {})
        
        if not topic_segments and not topic_shifts:
            return "## ğŸ·ï¸ ì£¼ì œ ë¶„ì„\n\n*ì£¼ì œ ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.*"
        
        section = "## ğŸ·ï¸ ì£¼ì œ ë¶„ì„"
        
        # Topic segments overview
        if topic_segments:
            section += f"\n\n### ì£¼ì œ ì„¸ê·¸ë¨¼íŠ¸"
            section += f"\n- **ì´ ì„¸ê·¸ë¨¼íŠ¸**: {len(topic_segments)}ê°œ"
            
            # Average segment characteristics
            avg_duration = sum([seg['duration_minutes'] for seg in topic_segments]) / len(topic_segments)
            avg_messages = sum([seg['message_count'] for seg in topic_segments]) / len(topic_segments)
            
            section += f"\n- **í‰ê·  ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´**: {avg_duration:.1f}ë¶„"
            section += f"\n- **ì„¸ê·¸ë¨¼íŠ¸ë‹¹ í‰ê·  ë©”ì‹œì§€**: {avg_messages:.1f}ê°œ"
            
            # Top segments by activity
            top_segments = sorted(topic_segments, key=lambda x: x['message_count'], reverse=True)[:3]
            section += f"\n\n### ì£¼ìš” ëŒ€í™” ì£¼ì œ"
            
            for i, segment in enumerate(top_segments, 1):
                keywords = ', '.join(segment.get('keywords', [])[:5])
                section += f"\n**{i}. {segment.get('summary', 'No summary')}**"
                section += f"\n- í‚¤ì›Œë“œ: {keywords}"
                section += f"\n- ë©”ì‹œì§€ ìˆ˜: {segment['message_count']}ê°œ"
                section += f"\n- ì§€ì† ì‹œê°„: {segment['duration_minutes']:.1f}ë¶„\n"
        
        # Topic shift analysis
        if topic_shifts:
            section += f"\n\n### ì£¼ì œ ì „í™˜ íŒ¨í„´"
            section += f"\n- **ì¼í‰ê·  ì£¼ì œ ì „í™˜**: {topic_shifts.get('total_shifts', 0) / max(1, topic_shifts.get('analysis_days', 1)):.1f}íšŒ"
            
            shift_intervals = topic_shifts.get('shift_intervals', {})
            if shift_intervals:
                section += f"\n- **í‰ê·  ì „í™˜ ê°„ê²©**: {shift_intervals.get('avg_minutes', 0):.1f}ë¶„"
        
        return section
    
    def _generate_fun_metrics_section(self, analysis_results: Dict[str, Any]) -> str:
        """Generate fun metrics section"""
        
        fun_metrics = analysis_results.get('fun_metrics', {})
        
        if not fun_metrics:
            return "## ğŸ¯ ì¬ë¯¸ìˆëŠ” ì§€í‘œ\n\n*ì¬ë¯¸ ì§€í‘œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.*"
        
        section = "## ğŸ¯ ì¬ë¯¸ìˆëŠ” ì§€í‘œ"
        
        # Participation inequality
        participation = fun_metrics.get('participation_inequality', {})
        if participation:
            section += f"\n\n### ì°¸ì—¬ ë¶ˆí‰ë“±"
            section += f"\n- **ì§€ë‹ˆ ê³„ìˆ˜**: {participation.get('gini_coefficient', 0):.3f} ({participation.get('interpretation', 'N/A')})"
            section += f"\n- **ìƒìœ„ 20% ì‚¬ìš©ìì˜ ë©”ì‹œì§€ ë¹„ì¤‘**: {participation.get('top_20_percent_share', 0):.1f}%"
            section += f"\n- **ìµœê³  í™œë™ ì‚¬ìš©ì**: {participation.get('most_active_user', 'N/A')} ({participation.get('top_user_share', 0):.1f}%)"
        
        # Reply latency
        reply_latency = fun_metrics.get('reply_latency', {})
        overall_stats = reply_latency.get('overall_stats', {})
        if overall_stats and 'error' not in overall_stats:
            section += f"\n\n### ë‹µì¥ ì†ë„"
            section += f"\n- **í‰ê·  ë‹µì¥ ì‹œê°„**: {overall_stats.get('mean_minutes', 0):.1f}ë¶„"
            section += f"\n- **ì¤‘ê°„ ë‹µì¥ ì‹œê°„**: {overall_stats.get('median_minutes', 0):.1f}ë¶„"
            section += f"\n- **ê°€ì¥ ë¹ ë¥¸ ë‹µì¥**: {overall_stats.get('min_minutes', 0):.1f}ë¶„"
            
            # Reply speed categories
            categories = reply_latency.get('reply_speed_categories', {})
            if categories:
                instant = categories.get('instant_replies', {})
                section += f"\n- **ì¦‰ì„ ë‹µì¥ (1ë¶„ ì´ë‚´)**: {instant.get('percent', 0):.1f}%"
        
        # Activity streaks
        streaks = fun_metrics.get('activity_streaks', {})
        if streaks:
            section += f"\n\n### í™œë™ íŒ¨í„´"
            section += f"\n- **ìµœì¥ ì—°ì† ë©”ì‹œì§€**: {streaks.get('longest_message_streak', 0)}ê°œ"
            section += f"\n- **ìµœì¥ ì—°ì† í™œë™ì¼**: {streaks.get('longest_daily_streak', 0)}ì¼"
        
        # Night chat analysis
        night_chat = fun_metrics.get('night_chat_analysis', {})
        if night_chat and 'error' not in night_chat:
            section += f"\n\n### ë°¤ìƒ˜ ì±„íŒ…"
            section += f"\n- **ì‹¬ì•¼ ì±„íŒ… ë¹„ìœ¨**: {night_chat.get('night_chat_percentage', 0):.1f}%"
            section += f"\n- **ê°€ì¥ í™œë°œí•œ ì‹¬ì•¼ ì‹œê°„**: {night_chat.get('peak_night_hour', 'N/A')}"
        
        return section
    
    def _generate_mention_analysis_section(self, analysis_results: Dict[str, Any]) -> str:
        """Generate mention analysis section"""
        
        mention_analysis = analysis_results.get('mention_analysis', {})
        
        section = "## @ ë©˜ì…˜ ë¶„ì„"
        section += f"\n\n### ë©˜ì…˜ ì‚¬ìš© íŒ¨í„´"
        
        summary = mention_analysis.get('analysis_summary', {})
        stats = mention_analysis.get('mention_statistics', {})
        
        if summary.get('total_mentions', 0) > 0:
            section += f"\n- **ì´ ë©˜ì…˜ ìˆ˜**: {summary['total_mentions']}ê°œ"
            section += f"\n- **ë©˜ì…˜ ë¹„ìœ¨**: {summary['mention_rate']:.1f}% (ì „ì²´ ë©”ì‹œì§€ ëŒ€ë¹„)"
            section += f"\n- **ë©˜ì…˜ ì‚¬ìš©ì**: {summary['unique_mentioners']}ëª…"
            section += f"\n- **ë©˜ì…˜ë°›ì€ ì‚¬ìš©ì**: {summary['unique_mentioned']}ëª…"
            
            # Top mentioners
            if stats.get('top_mentioners'):
                section += f"\n\n### ê°€ì¥ ë§ì´ ë©˜ì…˜í•˜ëŠ” ì‚¬ìš©ì"
                for user_stat in stats['top_mentioners'][:5]:
                    section += f"\n- **{user_stat['user']}**: {user_stat['count']}íšŒ ({user_stat['percentage']:.1f}%)"
            
            # Top mentioned
            if stats.get('top_mentioned'):
                section += f"\n\n### ê°€ì¥ ë§ì´ ë©˜ì…˜ë°›ëŠ” ì‚¬ìš©ì"
                for user_stat in stats['top_mentioned'][:5]:
                    section += f"\n- **{user_stat['user']}**: {user_stat['count']}íšŒ ({user_stat['percentage']:.1f}%)"
            
            # Most active pairs
            if stats.get('most_active_pairs'):
                section += f"\n\n### í™œë°œí•œ ë©˜ì…˜ ê´€ê³„"
                for pair in stats['most_active_pairs'][:3]:
                    section += f"\n- **{pair['mentioner']} â†’ {pair['mentioned']}**: {pair['count']}íšŒ"
            
            # Network analysis
            networks = mention_analysis.get('mention_networks', {})
            if networks.get('reciprocal_mentions'):
                section += f"\n\n### ìƒí˜¸ ë©˜ì…˜ ê´€ê³„"
                section += f"\n- **ìƒí˜¸ ë©˜ì…˜í•˜ëŠ” ìŒ**: {len(networks['reciprocal_mentions'])}ìŒ"
                for pair in networks['reciprocal_mentions'][:3]:
                    section += f"\n- **{pair['user1']} â†” {pair['user2']}**: "
                    section += f"{pair['mentions_1_to_2']}íšŒ / {pair['mentions_2_to_1']}íšŒ"
            
            # Time patterns
            patterns = mention_analysis.get('mention_patterns', {})
            if patterns.get('peak_mention_hour') is not None:
                section += f"\n\n### ë©˜ì…˜ ì‹œê°„ íŒ¨í„´"
                section += f"\n- **ê°€ì¥ í™œë°œí•œ ì‹œê°„**: {patterns['peak_mention_hour']}ì‹œ"
                
            if patterns.get('peak_mention_day'):
                section += f"\n- **ê°€ì¥ í™œë°œí•œ ìš”ì¼**: {patterns['peak_mention_day']}"
            
            # Context analysis
            context_analysis = patterns.get('mention_context_analysis', {})
            if context_analysis.get('context_percentages'):
                section += f"\n\n### ë©˜ì…˜ ì‚¬ìš© ëª©ì "
                for context, percentage in context_analysis['context_percentages'].items():
                    if percentage > 0:
                        korean_context = {
                            'questions': 'ì§ˆë¬¸',
                            'requests': 'ë¶€íƒ/ìš”ì²­', 
                            'greetings': 'ì¸ì‚¬',
                            'urgent': 'ê¸´ê¸‰',
                            'discussions': 'í† ë¡ /ì˜ê²¬'
                        }.get(context, context)
                        section += f"\n- **{korean_context}**: {percentage:.1f}%"
        else:
            section += f"\n\nì´ ëŒ€í™”ì—ì„œëŠ” @ ë©˜ì…˜ ê¸°ëŠ¥ì´ ì‚¬ìš©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        return section
    
    def _generate_context_flow_section(self, analysis_results: Dict[str, Any]) -> str:
        """Generate context flow analysis section"""
        
        mention_analysis = analysis_results.get('mention_analysis', {})
        context_analysis = mention_analysis.get('context_analysis', {})
        
        section = "## ğŸ”„ ëŒ€í™” ë§¥ë½ ë¶„ì„"
        
        response_analysis = context_analysis.get('response_analysis', {})
        if response_analysis.get('total_mentions', 0) > 0:
            section += f"\n\n### ë©˜ì…˜ ì‘ë‹µ ë¶„ì„"
            section += f"\n- **ì „ì²´ ì‘ë‹µë¥ **: {response_analysis.get('overall_response_rate', 0):.1f}%"
            section += f"\n- **í‰ê·  ì‘ë‹µ ì‹œê°„**: {response_analysis.get('average_response_time_minutes', 0):.1f}ë¶„"
            section += f"\n- **ì‘ë‹µë°›ì€ ë©˜ì…˜**: {response_analysis.get('mentions_with_response', 0)}ê°œ / {response_analysis.get('total_mentions', 0)}ê°œ"
            
            # User effectiveness
            user_effectiveness = response_analysis.get('user_effectiveness', [])
            if user_effectiveness:
                section += f"\n\n### ë©˜ì…˜ íš¨ê³¼ì ì¸ ì‚¬ìš©ì"
                for user_stat in user_effectiveness[:5]:
                    section += f"\n- **{user_stat['user']}**: {user_stat['effectiveness_rate']}% ì„±ê³µë¥  ({user_stat['successful_mentions']}/{user_stat['total_mentions']})"
        
        # Topic distribution
        topic_distribution = context_analysis.get('topic_distribution', {})
        top_topics = topic_distribution.get('top_topics', [])
        if top_topics:
            section += f"\n\n### ë©˜ì…˜ ëŒ€í™” ì£¼ì œ"
            for topic_info in top_topics[:5]:
                section += f"\n- **{topic_info['topic']}**: {topic_info['count']}íšŒ"
        
        # Context patterns
        context_patterns = context_analysis.get('context_patterns', {})
        pattern_percentages = context_patterns.get('pattern_percentages', {})
        if pattern_percentages:
            section += f"\n\n### ë©˜ì…˜ ì‚¬ìš© íŒ¨í„´"
            section += f"\n*ì°¸ê³ : í•˜ë‚˜ì˜ ë©˜ì…˜ì´ ì—¬ëŸ¬ íŒ¨í„´ì— ë™ì‹œì— í•´ë‹¹í•  ìˆ˜ ìˆì–´ ì´í•©ì´ 100%ë¥¼ ì´ˆê³¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤*"
            korean_pattern_names = {
                'mention_in_question': 'ì§ˆë¬¸ì‹œ ë©˜ì…˜',
                'mention_in_request': 'ë¶€íƒì‹œ ë©˜ì…˜', 
                'mention_in_greeting': 'ì¸ì‚¬ì‹œ ë©˜ì…˜',
                'mention_in_discussion': 'í† ë¡ ì‹œ ë©˜ì…˜',
                'mention_after_silence': 'ì¹¨ë¬µ í›„ ë©˜ì…˜',
                'mention_in_group_chat': 'ê·¸ë£¹ ëŒ€í™” ì¤‘ ë©˜ì…˜'
            }
            
            for pattern, percentage in pattern_percentages.items():
                if percentage > 0:
                    korean_name = korean_pattern_names.get(pattern, pattern)
                    section += f"\n- **{korean_name}**: {percentage}%"
            
            most_common = context_patterns.get('most_common_pattern')
            if most_common:
                korean_most_common = korean_pattern_names.get(most_common, most_common)
                section += f"\n\n**ê°€ì¥ ì¼ë°˜ì ì¸ ë©˜ì…˜ íŒ¨í„´**: {korean_most_common}"
        
        return section
    
    def _generate_topic_sentiment_section(self, analysis_results: Dict[str, Any]) -> str:
        """Generate topic sentiment analysis section"""
        
        topic_sentiment = analysis_results.get('topic_sentiment', {})
        
        section = "## ğŸ­ í† í”½ë³„ ê°ì • ë¶„ì„"
        
        total_segments = topic_sentiment.get('total_segments', 0)
        if total_segments == 0:
            section += "\n\në¶„ì„í•  ìˆ˜ ìˆëŠ” í† í”½ ì„¸ê·¸ë¨¼íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."
            return section
        
        # Overall sentiment distribution
        sentiment_dist = topic_sentiment.get('sentiment_distribution', {})
        overall_mood = topic_sentiment.get('overall_mood', 'neutral')
        
        mood_names = {
            'positive': 'ê¸ì •ì ',
            'negative': 'ë¶€ì •ì ',
            'neutral': 'ì¤‘ë¦½ì '
        }
        
        section += f"\n\n### ì „ë°˜ì ì¸ ê°ì • ë¶„ìœ„ê¸°"
        section += f"\n- **ì£¼ëœ ë¶„ìœ„ê¸°**: {mood_names.get(overall_mood, overall_mood)}"
        section += f"\n- **ì´ í† í”½ ì„¸ê·¸ë¨¼íŠ¸**: {total_segments}ê°œ"
        
        if sentiment_dist:
            section += f"\n\n### ê°ì • ë¶„í¬"
            for sentiment, percentage in sentiment_dist.items():
                korean_sentiment = mood_names.get(sentiment, sentiment)
                section += f"\n- **{korean_sentiment}**: {percentage:.1f}%"
        
        # Mood transitions
        mood_transitions = topic_sentiment.get('mood_transitions', {})
        stability = mood_transitions.get('stability', 1.0)
        total_changes = mood_transitions.get('total_changes', 0)
        
        section += f"\n\n### ê°ì • ë³€í™” íŒ¨í„´"
        section += f"\n- **ê°ì • ì•ˆì •ì„±**: {stability:.1f} (1.0ì´ ê°€ì¥ ì•ˆì •ì )"
        section += f"\n- **ê°ì • ë³€í™” íšŸìˆ˜**: {total_changes}íšŒ"
        
        if stability > 0.8:
            stability_desc = "ë§¤ìš° ì•ˆì •ì ì¸ ê°ì • íë¦„"
        elif stability > 0.6:
            stability_desc = "ë³´í†µ ìˆ˜ì¤€ì˜ ê°ì • ì•ˆì •ì„±"
        elif stability > 0.4:
            stability_desc = "ë‹¤ì†Œ ë³€í™”ê°€ ë§ì€ ê°ì • íë¦„"
        else:
            stability_desc = "ë§¤ìš° ì—­ë™ì ì¸ ê°ì • ë³€í™”"
        
        section += f"\n- **ê°ì • íë¦„ íŠ¹ì„±**: {stability_desc}"
        
        most_common_transition = mood_transitions.get('most_common_transition')
        if most_common_transition and most_common_transition != "stable mood":
            section += f"\n- **ê°€ì¥ í”í•œ ê°ì • ë³€í™”**: {most_common_transition}"
        
        # Context patterns
        context_patterns = topic_sentiment.get('context_patterns', {})
        context_dist = context_patterns.get('context_distribution', {})
        most_common_context = context_patterns.get('most_common_context')
        
        if context_dist:
            section += f"\n\n### ëŒ€í™” ë§¥ë½ ë¶„í¬"
            
            context_names = {
                'discussion': 'í† ë¡ /ë…¼ì˜',
                'planning': 'ê³„íš ìˆ˜ë¦½',
                'problem_solving': 'ë¬¸ì œ í•´ê²°',
                'casual_chat': 'ì¼ìƒ ëŒ€í™”',
                'emotional_support': 'ê°ì •ì  ì§€ì§€',
                'information_sharing': 'ì •ë³´ ê³µìœ '
            }
            
            # Sort by count and show top contexts
            sorted_contexts = sorted(context_dist.items(), key=lambda x: x[1], reverse=True)
            for context, count in sorted_contexts[:5]:
                korean_context = context_names.get(context, context)
                percentage = (count / total_segments) * 100
                section += f"\n- **{korean_context}**: {count}íšŒ ({percentage:.1f}%)"
        
        # Average intensity by context
        avg_intensities = context_patterns.get('avg_intensity_by_context', {})
        if avg_intensities:
            section += f"\n\n### ë§¥ë½ë³„ ê°ì • ê°•ë„"
            sorted_intensities = sorted(avg_intensities.items(), key=lambda x: x[1], reverse=True)
            
            context_names = {
                'discussion': 'í† ë¡ /ë…¼ì˜',
                'planning': 'ê³„íš ìˆ˜ë¦½', 
                'problem_solving': 'ë¬¸ì œ í•´ê²°',
                'casual_chat': 'ì¼ìƒ ëŒ€í™”',
                'emotional_support': 'ê°ì •ì  ì§€ì§€',
                'information_sharing': 'ì •ë³´ ê³µìœ '
            }
            
            for context, intensity in sorted_intensities[:3]:
                korean_context = context_names.get(context, context)
                intensity_level = "ë†’ìŒ" if intensity > 0.3 else "ë³´í†µ" if intensity > 0.1 else "ë‚®ìŒ"
                section += f"\n- **{korean_context}**: {intensity:.2f} ({intensity_level})"
        
        # Segment examples
        segment_analysis = topic_sentiment.get('segment_analysis', [])
        if segment_analysis:
            # Find interesting segments to highlight
            positive_segments = [s for s in segment_analysis if s.get('overall_sentiment') == 'positive']
            negative_segments = [s for s in segment_analysis if s.get('overall_sentiment') == 'negative']
            
            if positive_segments:
                # Most positive segment
                most_positive = max(positive_segments, key=lambda x: x.get('sentiment_scores', {}).get('positive', 0))
                section += f"\n\n### ê°€ì¥ ê¸ì •ì ì¸ í† í”½"
                section += f"\n- **ê¸°ê°„**: {most_positive.get('start_time', '')} ~ {most_positive.get('end_time', '')}"
                section += f"\n- **ë©”ì‹œì§€ ìˆ˜**: {most_positive.get('message_count', 0)}ê°œ"
                section += f"\n- **ì°¸ì—¬ì**: {most_positive.get('participant_count', 0)}ëª…"
                
                key_phrases = most_positive.get('key_phrases', [])
                if key_phrases:
                    section += f"\n- **ì£¼ìš” í‚¤ì›Œë“œ**: {', '.join(key_phrases[:5])}"
            
            if negative_segments:
                # Most negative segment
                most_negative = max(negative_segments, key=lambda x: x.get('sentiment_scores', {}).get('negative', 0))
                section += f"\n\n### ê°€ì¥ ë¶€ì •ì ì¸ í† í”½"
                section += f"\n- **ê¸°ê°„**: {most_negative.get('start_time', '')} ~ {most_negative.get('end_time', '')}"
                section += f"\n- **ë©”ì‹œì§€ ìˆ˜**: {most_negative.get('message_count', 0)}ê°œ"
                section += f"\n- **ì°¸ì—¬ì**: {most_negative.get('participant_count', 0)}ëª…"
                
                key_phrases = most_negative.get('key_phrases', [])
                if key_phrases:
                    section += f"\n- **ì£¼ìš” í‚¤ì›Œë“œ**: {', '.join(key_phrases[:5])}"
        
        return section
    
    def _generate_advanced_ai_section(self, analysis_results: Dict[str, Any]) -> str:
        """Generate advanced AI analysis section (SPLADE, Ollama)"""
        
        section = "## ğŸ¤– ê³ ê¸‰ AI ë¶„ì„"
        
        # SPLADE sparse retrieval analysis
        sparse_index_info = analysis_results.get('sparse_index_info', {})
        if sparse_index_info:
            section += f"\n\n### SPLADE í¬ì†Œ ê²€ìƒ‰ ë¶„ì„"
            section += f"\n- **ëª¨ë¸**: {sparse_index_info.get('model_name', 'naver/splade-cocondenser-ensembledistil')}"
            section += f"\n- **ì¸ë±ìŠ¤ëœ ë¬¸ì„œ ìˆ˜**: {sparse_index_info.get('total_documents', 0):,}ê°œ"
            section += f"\n- **í‰ê·  í¬ì†Œì„±**: {sparse_index_info.get('avg_sparsity', 0):.3f}"
            section += f"\n- **ë²¡í„° ì°¨ì›**: {sparse_index_info.get('vector_dimension', 0):,}ì°¨ì›"
            
            # Top sparse terms
            top_terms = sparse_index_info.get('top_sparse_terms', [])
            if top_terms:
                section += f"\n\n**ì£¼ìš” í¬ì†Œ íŠ¹ì§• í† í°:**"
                for term_info in top_terms[:10]:
                    section += f"\n- **{term_info.get('term', '')}**: {term_info.get('weight', 0):.3f}"
            
            # Search capabilities
            search_stats = sparse_index_info.get('search_stats', {})
            if search_stats:
                section += f"\n\n**ê²€ìƒ‰ ì„±ëŠ¥:**"
                section += f"\n- **ê²€ìƒ‰ ê°€ëŠ¥ í† í° ìˆ˜**: {search_stats.get('searchable_tokens', 0):,}ê°œ"
                section += f"\n- **ê³ ìœ  í† í° ìˆ˜**: {search_stats.get('unique_tokens', 0):,}ê°œ"
                section += f"\n- **í‰ê·  ê²€ìƒ‰ ì‘ë‹µ ì‹œê°„**: {search_stats.get('avg_search_time_ms', 0):.1f}ms"
        
        # Ollama model analysis
        advanced_topics = analysis_results.get('advanced_topics', {})
        if advanced_topics:
            section += f"\n\n### Ollama ëª¨ë¸ ë¶„ì„ (oss:20b)"
            
            model_info = advanced_topics.get('model_info', {})
            section += f"\n- **ëª¨ë¸**: {model_info.get('name', 'oss:20b')}"
            section += f"\n- **ëª¨ë¸ í¬ê¸°**: {model_info.get('size', 'Unknown')}"
            section += f"\n- **ë¶„ì„ëœ ì„¸ê·¸ë¨¼íŠ¸**: {advanced_topics.get('analyzed_segments', 0)}ê°œ"
            
            # Topic insights
            insights = advanced_topics.get('insights', [])
            if insights:
                section += f"\n\n**AI ìƒì„± ì¸ì‚¬ì´íŠ¸:**"
                for i, insight in enumerate(insights[:5], 1):
                    section += f"\n{i}. {insight}"
            
            # Topic themes discovered by LLM
            themes = advanced_topics.get('discovered_themes', [])
            if themes:
                section += f"\n\n**ë°œê²¬ëœ ì£¼ì œ íŒ¨í„´:**"
                for theme in themes[:5]:
                    section += f"\n- **{theme.get('theme', '')}**: {theme.get('description', '')}"
                    section += f"  - ë¹ˆë„: {theme.get('frequency', 0)}íšŒ"
            
            # Conversation quality analysis
            quality_analysis = advanced_topics.get('conversation_quality', {})
            if quality_analysis:
                section += f"\n\n**ëŒ€í™” í’ˆì§ˆ ë¶„ì„:**"
                section += f"\n- **ì „ë°˜ì  í’ˆì§ˆ ì ìˆ˜**: {quality_analysis.get('overall_score', 0):.2f}/5.0"
                section += f"\n- **ì£¼ì œ ê¹Šì´**: {quality_analysis.get('topic_depth', 'Unknown')}"
                section += f"\n- **ì°¸ì—¬ ê· í˜•**: {quality_analysis.get('participation_balance', 'Unknown')}"
                section += f"\n- **ê°ì •ì  í†¤**: {quality_analysis.get('emotional_tone', 'Unknown')}"
            
            # Processing performance
            processing_stats = advanced_topics.get('processing_stats', {})
            if processing_stats:
                section += f"\n\n**ì²˜ë¦¬ ì„±ëŠ¥:**"
                section += f"\n- **ì´ ì²˜ë¦¬ ì‹œê°„**: {processing_stats.get('total_time_seconds', 0):.1f}ì´ˆ"
                section += f"\n- **í† í° ì²˜ë¦¬ìœ¨**: {processing_stats.get('tokens_per_second', 0):.0f} tokens/sec"
                section += f"\n- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: {processing_stats.get('peak_memory_mb', 0):.1f}MB"
        
        # Combined analysis insights
        if sparse_index_info and advanced_topics:
            section += f"\n\n### í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ê²°ê³¼"
            section += f"\ní¬ì†Œ ê²€ìƒ‰(SPLADE)ê³¼ ëŒ€í™”í˜• AI(Ollama)ë¥¼ ê²°í•©í•œ ë¶„ì„ì„ í†µí•´ ë‹¤ìŒê³¼ ê°™ì€ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí–ˆìŠµë‹ˆë‹¤:"
            
            # Example hybrid insights (this would be computed in the actual analysis pipeline)
            hybrid_insights = [
                "SPLADE í¬ì†Œ íŠ¹ì§•ê³¼ Ollama ì£¼ì œ ë¶„ì„ì´ ì¼ì¹˜í•˜ëŠ” í•µì‹¬ í† í”½ ë°œê²¬",
                "AI ëª¨ë¸ì´ ì‹ë³„í•œ ê°ì • íŒ¨í„´ê³¼ í¬ì†Œ ë²¡í„° ê°€ì¤‘ì¹˜ ê°„ ìƒê´€ê´€ê³„ í™•ì¸",
                "ê²€ìƒ‰ ê¸°ë°˜ ë¬¸ì„œ ìœ ì‚¬ì„±ê³¼ LLM ê¸°ë°˜ ì˜ë¯¸ ë¶„ì„ì˜ ìƒí˜¸ ë³´ì™„ì  ê²°ê³¼"
            ]
            
            for insight in hybrid_insights:
                section += f"\n- {insight}"
        
        if not sparse_index_info and not advanced_topics:
            section += f"\n\n*SPLADE ë˜ëŠ” Ollama ë¶„ì„ì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. --use-splade ë° --use-ollama ì˜µì…˜ì„ ì‚¬ìš©í•˜ì—¬ ê³ ê¸‰ AI ë¶„ì„ì„ ì‹¤í–‰í•˜ì„¸ìš”.*"
        
        return section
    
    def _generate_visualizations_section(self, viz_files: Dict[str, str]) -> str:
        """Generate visualizations section"""
        
        section = "## ğŸ“Š ì‹œê°í™”"
        
        viz_descriptions = {
            'heatmap': ('ì‹œê°„ëŒ€ë³„ Ã— ìš”ì¼ë³„ í™œë™ íˆíŠ¸ë§µ', 'í•˜ë£¨ ì¤‘ ì–´ëŠ ì‹œê°„ëŒ€ì— ê°€ì¥ í™œë°œí•œì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.'),
            'timeline': ('ì¼ë³„ ë©”ì‹œì§€ ìˆ˜ ì¶”ì´', 'ì‹œê°„ì— ë”°ë¥¸ ëŒ€í™”ëŸ‰ ë³€í™”ë¥¼ ì¶”ì í•©ë‹ˆë‹¤.'),
            'topic_timeline': ('ì£¼ì œ ì„¸ê·¸ë¨¼íŠ¸ íƒ€ì„ë¼ì¸', 'ëŒ€í™” ì£¼ì œê°€ ì‹œê°„ì— ë”°ë¼ ì–´ë–»ê²Œ ë³€í–ˆëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.'),
            'wordcloud': ('ì „ì—­ ì›Œë“œí´ë¼ìš°ë“œ', 'ê°€ì¥ ìì£¼ ì‚¬ìš©ëœ ë‹¨ì–´ë“¤ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.'),
            'user_activity': ('ì‚¬ìš©ì í™œë™ ë¶„í¬', 'ê° ì‚¬ìš©ìì˜ ì°¸ì—¬ë„ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.'),
            'reply_latency': ('ë‹µì¥ ì§€ì—° ì‹œê°„ ë¶„í¬', 'ì‚¬ìš©ìë“¤ì˜ ë‹µì¥ ì†ë„ íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤.'),
            'network_graph': ('ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ë„¤íŠ¸ì›Œí¬', 'ì‚¬ìš©ìë“¤ ê°„ì˜ ëŒ€í™” ìƒí˜¸ì‘ìš© ê´€ê³„ë¥¼ ë„¤íŠ¸ì›Œí¬ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.'),
            'mention_network': ('ë©˜ì…˜ ë„¤íŠ¸ì›Œí¬', '@ ë©˜ì…˜ì„ í†µí•œ ì‚¬ìš©ì ê°„ ì–¸ê¸‰ ê´€ê³„ë¥¼ ë„¤íŠ¸ì›Œí¬ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤.')
        }
        
        for viz_key, file_path in viz_files.items():
            if viz_key in viz_descriptions:
                title, description = viz_descriptions[viz_key]
                relative_path = Path(file_path).name
                section += f"\n\n### {title}"
                section += f"\n{description}"
                section += f"\n\n![{title}](figures/{relative_path})"
        
        return section
    
    def _generate_technical_details(self, analysis_results: Dict[str, Any]) -> str:
        """Generate technical details section"""
        
        section = "## ğŸ”§ ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­"
        
        # Model information
        if 'embeddings_info' in analysis_results:
            embeddings_info = analysis_results['embeddings_info']
            model_info = embeddings_info.get('model_info', {})
            section += f"\n\n### ì„ë² ë”© ëª¨ë¸"
            section += f"\n- **ëª¨ë¸ëª…**: {model_info.get('model_name', 'N/A')}"
            section += f"\n- **ì°¨ì›ìˆ˜**: {model_info.get('dimension', 'N/A')}"
            section += f"\n- **ìœˆë„ìš° í¬ê¸°**: {embeddings_info.get('window_size', 'N/A')}"
        
        # Processing parameters
        section += f"\n\n### ë¶„ì„ íŒŒë¼ë¯¸í„°"
        section += f"\n- **ëŒ€í™” í„´ êµ¬ë¶„ ì‹œê°„**: {self.config.window_minutes}ë¶„"
        section += f"\n- **ì£¼ì œ ì„¸ê·¸ë¨¼íŠ¸ ìœˆë„ìš°**: {self.config.topic_window_size}ê°œ ë©”ì‹œì§€"
        section += f"\n- **ìœ ì‚¬ë„ ì„ê³„ê°’**: {self.config.similarity_threshold}"
        
        # Data quality metrics
        if 'data_validation' in analysis_results:
            validation = analysis_results['data_validation']
            section += f"\n\n### ë°ì´í„° í’ˆì§ˆ"
            section += f"\n- **ìœ íš¨ì„±**: {'í†µê³¼' if validation.get('is_valid') else 'ì‹¤íŒ¨'}"
            section += f"\n- **ê²½ê³  ìˆ˜**: {len(validation.get('warnings', []))}"
            section += f"\n- **ì˜¤ë¥˜ ìˆ˜**: {len(validation.get('errors', []))}"
        
        return section
    
    def _generate_insights_section(self, analysis_results: Dict[str, Any]) -> str:
        """Generate insights and recommendations section"""
        
        insights = self._extract_key_insights(analysis_results)
        recommendations = self._generate_recommendations(analysis_results)
        
        section = "## ğŸ’¡ ì¸ì‚¬ì´íŠ¸ ë° ê¶Œì¥ì‚¬í•­"
        
        section += f"\n\n### ì£¼ìš” ë°œê²¬ì‚¬í•­"
        for i, insight in enumerate(insights[:5], 1):
            section += f"\n{i}. {insight}"
        
        if recommendations:
            section += f"\n\n### ê¶Œì¥ì‚¬í•­"
            for i, rec in enumerate(recommendations[:3], 1):
                section += f"\n{i}. {rec}"
        
        # Limitations
        section += f"\n\n### ë¶„ì„ì˜ í•œê³„"
        section += f"\n- í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶„ì„ìœ¼ë¡œ ì´ë¯¸ì§€, íŒŒì¼ ë“±ì€ ì œì™¸ë¨"
        section += f"\n- ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” í•„í„°ë§ë˜ì–´ ì‹¤ì œ ëŒ€í™”ë§Œ ë¶„ì„ë¨"
        section += f"\n- ì£¼ì œ ë¶„ë¥˜ëŠ” í‚¤ì›Œë“œ ê¸°ë°˜ì˜ ë‹¨ìˆœí•œ ë°©ë²• ì‚¬ìš©"
        
        return section
    
    def _generate_footer(self) -> str:
        """Generate report footer"""
        
        return f"""
---

*ì´ ë¦¬í¬íŠ¸ëŠ” Kakao Analyzerë¥¼ ì‚¬ìš©í•˜ì—¬ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*

ğŸ¤– Generated with [Claude Code](https://claude.ai/code)"""
    
    def _extract_key_insights(self, analysis_results: Dict[str, Any]) -> List[str]:
        """Extract key insights from analysis results"""
        
        insights = []
        
        # Basic stats insights
        basic_stats = analysis_results.get('basic_stats', {})
        summary_stats = basic_stats.get('summary', {})
        
        if summary_stats and 'error' not in summary_stats:
            activity_summary = summary_stats.get('activity_summary', {})
            
            # Most active user insight
            most_active = activity_summary.get('most_active_user', '')
            most_active_percent = activity_summary.get('most_active_user_percent', 0)
            if most_active and most_active_percent > 30:
                insights.append(f"{most_active}ê°€ ì „ì²´ ëŒ€í™”ì˜ {most_active_percent:.1f}%ë¥¼ ì£¼ë„í•˜ë©° ëŒ€í™”ë°©ì˜ ì¤‘ì‹¬ ì—­í• ")
            
            # Messages per day
            msg_per_day = activity_summary.get('messages_per_day', 0)
            if msg_per_day > 50:
                insights.append(f"í•˜ë£¨ í‰ê·  {msg_per_day:.1f}ê°œ ë©”ì‹œì§€ë¡œ ë§¤ìš° í™œë°œí•œ ëŒ€í™”ë°©")
            elif msg_per_day > 20:
                insights.append(f"í•˜ë£¨ í‰ê·  {msg_per_day:.1f}ê°œ ë©”ì‹œì§€ë¡œ í™œë°œí•œ ëŒ€í™”ë°©")
        
        # Temporal insights
        temporal = basic_stats.get('temporal', {})
        if temporal:
            peak_activity = temporal.get('peak_activity', {})
            peak_hour = peak_activity.get('peak_hour', '')
            peak_day = peak_activity.get('peak_day', '')
            
            if peak_hour and peak_day:
                insights.append(f"{peak_day} {peak_hour}ì— ê°€ì¥ í™œë°œí•œ ëŒ€í™” íŒ¨í„´")
            
            time_periods = temporal.get('time_period_distribution', {})
            night_percent = time_periods.get('night_0_5', {}).get('percent', 0)
            if night_percent > 10:
                insights.append(f"ìƒˆë²½ ì‹œê°„ëŒ€ ë©”ì‹œì§€ ë¹„ì¤‘ì´ {night_percent:.1f}%ë¡œ ë°¤ìƒ˜ ì±„íŒ… ê²½í–¥")
        
        # Fun metrics insights
        fun_metrics = analysis_results.get('fun_metrics', {})
        
        # Gini coefficient
        participation = fun_metrics.get('participation_inequality', {})
        if participation:
            gini = participation.get('gini_coefficient', 0)
            if gini > 0.5:
                insights.append(f"ì§€ë‹ˆê³„ìˆ˜ {gini:.3f}ë¡œ ì†Œìˆ˜ ì‚¬ìš©ìê°€ ëŒ€í™”ë¥¼ ì£¼ë„í•˜ëŠ” ë¶ˆê· ë“± êµ¬ì¡°")
            elif gini < 0.3:
                insights.append(f"ì§€ë‹ˆê³„ìˆ˜ {gini:.3f}ë¡œ ëª¨ë“  ì°¸ì—¬ìê°€ ê³ ë¥´ê²Œ ì°¸ì—¬í•˜ëŠ” ê· ë“±í•œ êµ¬ì¡°")
        
        # Reply latency
        reply_latency = fun_metrics.get('reply_latency', {})
        overall_stats = reply_latency.get('overall_stats', {})
        if overall_stats and 'error' not in overall_stats:
            mean_reply = overall_stats.get('mean_minutes', 0)
            if mean_reply < 5:
                insights.append(f"í‰ê·  ë‹µì¥ ì‹œê°„ {mean_reply:.1f}ë¶„ìœ¼ë¡œ ë§¤ìš° ë¹ ë¥¸ ì†Œí†µ")
            elif mean_reply > 30:
                insights.append(f"í‰ê·  ë‹µì¥ ì‹œê°„ {mean_reply:.1f}ë¶„ìœ¼ë¡œ ëŠê¸‹í•œ ì†Œí†µ ìŠ¤íƒ€ì¼")
        
        # Topic segments
        topic_segments = analysis_results.get('topic_segments', [])
        if topic_segments:
            avg_duration = sum([seg['duration_minutes'] for seg in topic_segments]) / len(topic_segments)
            if avg_duration < 10:
                insights.append(f"í‰ê·  {avg_duration:.1f}ë¶„ì˜ ì§§ì€ ì£¼ì œ ì „í™˜ìœ¼ë¡œ ë‹¤ì–‘í•œ í™”ì œ")
            elif avg_duration > 60:
                insights.append(f"í‰ê·  {avg_duration:.1f}ë¶„ì˜ ê¹Šì´ ìˆëŠ” ì£¼ì œ ì§‘ì¤‘ ëŒ€í™”")
        
        return insights[:10]  # Limit to top 10 insights
    
    def _generate_recommendations(self, analysis_results: Dict[str, Any]) -> List[str]:
        """Generate recommendations based on analysis"""
        
        recommendations = []
        
        # Based on participation inequality
        fun_metrics = analysis_results.get('fun_metrics', {})
        participation = fun_metrics.get('participation_inequality', {})
        
        if participation:
            gini = participation.get('gini_coefficient', 0)
            if gini > 0.6:
                recommendations.append("ì°¸ì—¬ê°€ ì§‘ì¤‘ëœ êµ¬ì¡°ì´ë¯€ë¡œ, ì†Œê·¹ì  ì°¸ì—¬ìë“¤ì˜ ë°œì–¸ì„ ìœ ë„í•˜ëŠ” ë¶„ìœ„ê¸° ì¡°ì„± ê¶Œì¥")
            elif gini < 0.2:
                recommendations.append("ë§¤ìš° ê· ë“±í•œ ì°¸ì—¬ êµ¬ì¡°ë¡œ ê±´ê°•í•œ ì†Œí†µì´ ì´ë£¨ì–´ì§€ê³  ìˆìŒ")
        
        # Based on reply latency
        reply_latency = fun_metrics.get('reply_latency', {})
        overall_stats = reply_latency.get('overall_stats', {})
        if overall_stats and 'error' not in overall_stats:
            mean_reply = overall_stats.get('mean_minutes', 0)
            if mean_reply > 60:
                recommendations.append("ë‹µì¥ ì‹œê°„ì´ ê¸´ í¸ì´ë¯€ë¡œ ë” ì‹¤ì‹œê°„ ì†Œí†µì„ ìœ„í•œ í™œì„±í™” ë°©ì•ˆ ê³ ë ¤")
        
        # Based on activity patterns
        basic_stats = analysis_results.get('basic_stats', {})
        temporal = basic_stats.get('temporal', {})
        if temporal:
            time_periods = temporal.get('time_period_distribution', {})
            night_percent = time_periods.get('night_0_5', {}).get('percent', 0)
            if night_percent > 15:
                recommendations.append("ìƒˆë²½ ì‹œê°„ í™œë™ì´ ë†’ìœ¼ë¯€ë¡œ ê±´ê°•í•œ ìƒí™œ íŒ¨í„´ì„ ìœ„í•œ ëŒ€í™” ì‹œê°„ëŒ€ ì¡°ì • ê³ ë ¤")
        
        return recommendations
    
    def generate_qa_test_report(self, test_results: Dict[str, Any], 
                              output_path: Path) -> str:
        """Generate QA test results report"""
        
        self.logger.info("Generating QA test report...")
        
        report = f"""# í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¦¬í¬íŠ¸

**ìƒì„± ì¼ì‹œ**: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M')}  
**í…ŒìŠ¤íŠ¸ íŒŒì¼**: {test_results.get('test_file', 'test.csv')}

## í…ŒìŠ¤íŠ¸ ìš”ì•½

- **ì „ì²´ í…ŒìŠ¤íŠ¸**: {test_results.get('total_tests', 0)}ê°œ
- **í†µê³¼**: {test_results.get('passed_tests', 0)}ê°œ
- **ì‹¤íŒ¨**: {test_results.get('failed_tests', 0)}ê°œ
- **ì„±ê³µë¥ **: {test_results.get('success_rate', 0):.1f}%

## í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìƒì„¸

### âœ… í†µê³¼í•œ í…ŒìŠ¤íŠ¸
"""
        
        for test in test_results.get('passed', []):
            report += f"- {test['name']}: {test.get('description', '')}\n"
        
        report += f"\n### âŒ ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸\n"
        
        for test in test_results.get('failed', []):
            report += f"- {test['name']}: {test.get('error', 'Unknown error')}\n"
        
        report += f"""
## íŒŒì¼ ìƒì„± í™•ì¸

### í•„ìˆ˜ ì¶œë ¥ íŒŒì¼
"""
        
        required_files = test_results.get('file_checks', {})
        for file_path, exists in required_files.items():
            status = "âœ…" if exists else "âŒ"
            report += f"- {status} {file_path}\n"
        
        report += f"""
## ê¶Œì¥ì‚¬í•­

{chr(10).join([f"- {rec}" for rec in test_results.get('recommendations', [])])}

---

*ì´ í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ëŠ” ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*
"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        self.logger.info(f"QA test report saved to {output_path}")
        return str(output_path)